---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

**About Me**

I am a fifth-year PhD student at UT Austin co-advised by [Aryan Mokhtari](https://sites.utexas.edu/mokhtari/) and [Sanjay Shakkottai](https://sites.google.com/view/sanjay-shakkottai/home). I'm broadly interested in improving the learning abilities of machine learning models, especially in low-data and low-compute scenarios. This has led to work in a variety of areas, including federated learning, meta-learning, multi-task learning, contrastive learning, and most recently, in-context learning with and parameter-efficient fine-tuning of large language models. Before UT, I completed my undergrad at Princeton where I worked with [Yuxin Chen](https://yuxinchen2020.github.io/).

My email is liamc at utexas dot edu.

**I am graduating in May 2024 and am currently on the job market.**

<!---
My CV can be found [here](https://liamc2196.github.io/files/Liamc_CV_nov22.pdf) (updated 11/2022).
--->

## News

- **May 2024:** Our [paper](https://arxiv.org/pdf/2307.06887.pdf) on multi-task learning with two-layer ReLU networks was accepted at ICML 2024. 

- **February 2024:** New [paper](https://arxiv.org/pdf/2402.11639.pdf) on in-context learning with transformers with softmax-activated self-attention.

- **December 2023:** Our [paper](https://arxiv.org/pdf/2310.04627.pdf) was selected as a Best Paper at [FL@FM-NeurIPS’23](https://federated-learning.org/fl@fm-neurips-2023/)!

- **October 2023:** Our [paper](https://arxiv.org/pdf/2310.04627.pdf) on federated prompt tuning was selected for Oral Presentation at [FL@FM-NeurIPS’23](https://federated-learning.org/fl@fm-neurips-2023/).

- **Summer 2023:** I interned at Google Research, working with [Shanshan Wu](https://wushanshan.github.io/), [Sewoong Oh](https://homes.cs.washington.edu/~sewoong/), and [Khe Chai Sim](https://scholar.google.com/citations?user=jnU62sUAAAAJ&hl=en) on federated prompt tuning of large language models.

- **June 2023:** New [paper](https://arxiv.org/pdf/2307.06887.pdf) on multi-task learning with two-layer ReLU networks.

- **May 2023:** Our paper [InfoNCE Loss Provably Learns Cluster-Preserving Representations](https://arxiv.org/pdf/2302.07920.pdf) was accepted at COLT 2023.

- **October 2022:** I gave a [talk](https://sites.google.com/view/one-world-seminar-series-flow/archive/2022) on representation learning in federated learning at the Federated Learning One World (FLOW) Seminar.

- **Summer 2022:** I interned at Amazon Alexa under the supervision of [Jie Ding](https://jding.org/) and [Tanya Roosta](https://www.amazon.science/author/tanya-g-roosta). My project studied personalized federated learning with side information. Our [paper](https://openreview.net/forum?id=HRZjvFkX-faD) was accepted at [FL-NeurIPS'22](https://federated-learning.org/fl-neurips-2022/). 


## Preprints and Workshop Papers

For the most updated list of papers, please see my [Google Scholar profile](https://scholar.google.com/citations?user=MRLe02cAAAAJ&hl=en).

**In-Context Learning with Transformers: Softmax Attention Adapts to Function Lipschitzness**  
*LC\*, Advait Parulekar\*, Aryan Mokhtari, Sujay Sanghavi, Sanjay Shakkottai*  
\* co-first authors  
arXiv preprint  
[\[PDF\]](https://arxiv.org/pdf/2402.11639)

<!---{% for post in site.publications reversed %}
          {% include archive-single.html %}
     {% endfor %}--->

**Profit: Benchmarking Personalization and Robustness Trade-off in Federated Prompt Tuning**  
*LC, Shanshan Wu, Sewoong Oh, Khe Chai Sim*  
Workshop on Federated Learning in the Age of Foundation Models in Conjunction with NeurIPS 2023  **Best Paper**  
[\[PDF\]](https://arxiv.org/pdf/2310.04627.pdf) 

**PerFedSI: A Framework for Personalized Federated Learning with Side Information**  
*LC, Enmao Diao, Tanya Roosta, Jie Ding, Tao Zhang*  
Workshop on Federated Learning: Recent Advances and New Challenges in Conjunction with NeurIPS 2022  
[\[PDF\]](https://openreview.net/pdf?id=HRZjvFkX-faD) 


## Publications

**Provable Multi-Task Representation Learning by Two-Layer ReLU Neural Networks**  
*LC, Hamed Hassani, Mahdi Soltanolkotabi, Aryan Mokhtari, Sanjay Shakkottai*  
ICML 2024 **Oral Presentation**
[\[PDF\]](https://arxiv.org/pdf/2307.06887.pdf)
     
**InfoNCE Provably Learns Cluster-Preserving Representations**  
*Advait Parulekar, LC, Karthikeyan Shanmugam, Aryan Mokhtari, Sanjay Shakkottai*  
COLT 2023  
[\[PDF\]](https://arxiv.org/pdf/2302.07920.pdf)

**FedAvg with Fine-Tuning: Local Updates Lead to Representation Learning**  
*LC, Hamed Hassani, Aryan Mokhtari, Sanjay Shakkottai*  
NeurIPS 2022     
[\[PDF\]](https://arxiv.org/pdf/2205.13692.pdf)

**MAML and ANIL Provably Learn Representations**  
*LC, Aryan Mokhtari, Sewoong Oh, Sanjay Shakkottai*  
ICML 2022     
[\[PDF\]](https://arxiv.org/pdf/2202.03483.pdf)

**How does the Task Landscape Affect MAML Performance?**  
*LC, Aryan Mokhtari, Sanjay Shakkottai*  
CoLLAs 2022 *Oral Presentation*   
[\[PDF\]](https://arxiv.org/pdf/2010.14672.pdf)

**Exploiting Shared Representations for Personalized Federated
Learning**  
*LC, Hamed Hassani, Aryan Mokhtari, Sanjay Shakkottai*  
ICML 2021    
[\[PDF\]](https://arxiv.org/pdf/2102.07078.pdf) [\[Code\]](https://github.com/lgcollins/FedRep)

**Task-Robust Model-Agnostic Meta-Learning**  
*LC, Aryan Mokhtari, Sanjay Shakkottai*  
NeurIPS 2020    
[\[PDF\]](https://arxiv.org/abs/2002.04766.pdf) [\[Code\]](https://github.com/lgcollins/tr-maml)
