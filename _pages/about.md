---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

**About Me**

I am currently a Research Scientist at [Snap Research](https://research.snap.com/) on the [User Modeling and Personalization (UMaP)](https://research.snap.com/team/user-modeling-and-personalization.html) team led by [Neil Shah](https://nshah.net/). My work focuses on learning user representations for downstream recommendation tasks from sequential and multi-modal interaction data. 

Before Snap, I completed my PhD at UT Austin, where I studied a range of topics including in-context learning, multi-task learning and feature learning theory under the excellent supervision of [Aryan Mokhtari](https://sites.utexas.edu/mokhtari/) and [Sanjay Shakkottai](https://sites.google.com/view/sanjay-shakkottai/home). I was also fortunate to hold internships at Google Research and Amazon during my PhD. I completed my undergrad at Princeton University where I had the pleasure of working with [Yuxin Chen](https://yuxinchen2020.github.io/).

My email is lcollins2 at snap dot com.

**We are currently recruiting interns to work on a variety of projects in recommendation, graph learning, and user modeling for 2025. Start dates are flexible. If interested please apply [online](https://snap.submittable.com/submit) and send me an email.**

<!---
My CV can be found [here](https://liamc2196.github.io/files/Liamc_CV_nov22.pdf) (updated 11/2022).
--->

## News

- **September 2024:** Started working at Snap!

- **September 2024:** Our in-context learning [paper](https://www.arxiv.org/pdf/2402.11639.pdf) was selected for Spotlight Presentation at NeurIPS 2024.

- **June 2024:** Our multi-task learning [paper](https://arxiv.org/pdf/2307.06887.pdf) was selected for Oral Presentation at ICML 2024.

- **May 2024:** Our [paper](https://arxiv.org/pdf/2307.06887.pdf) on multi-task learning with two-layer ReLU networks was accepted at ICML 2024.

- **April 2024:** Defended my thesis! 

- **February 2024:** New [paper](https://arxiv.org/pdf/2402.11639.pdf) on in-context learning with transformers with softmax-activated self-attention.

- **December 2023:** Our [paper](https://arxiv.org/pdf/2310.04627.pdf) was selected as a Best Paper at [FL@FM-NeurIPS’23](https://federated-learning.org/fl@fm-neurips-2023/).

- **October 2023:** Our [paper](https://arxiv.org/pdf/2310.04627.pdf) on federated prompt tuning was selected for Oral Presentation at [FL@FM-NeurIPS’23](https://federated-learning.org/fl@fm-neurips-2023/).

- **Summer 2023:** I interned at Google Research, working with [Shanshan Wu](https://wushanshan.github.io/), [Sewoong Oh](https://homes.cs.washington.edu/~sewoong/), and [Khe Chai Sim](https://scholar.google.com/citations?user=jnU62sUAAAAJ&hl=en) on federated prompt tuning of large language models.

- **June 2023:** New [paper](https://arxiv.org/pdf/2307.06887.pdf) on multi-task learning with two-layer ReLU networks.

- **May 2023:** Our paper [InfoNCE Loss Provably Learns Cluster-Preserving Representations](https://arxiv.org/pdf/2302.07920.pdf) was accepted at COLT 2023.

- **October 2022:** I gave a [talk](https://sites.google.com/view/one-world-seminar-series-flow/archive/2022) on representation learning in federated learning at the Federated Learning One World (FLOW) Seminar.

- **Summer 2022:** I interned at Amazon Alexa under the supervision of [Jie Ding](https://jding.org/) and [Tanya Roosta](https://www.amazon.science/author/tanya-g-roosta). My project studied personalized federated learning with side information. Our [paper](https://openreview.net/forum?id=HRZjvFkX-faD) was accepted at [FL-NeurIPS'22](https://federated-learning.org/fl-neurips-2022/). 



<!---{% for post in site.publications reversed %}
          {% include archive-single.html %}
     {% endfor %}--->



## Papers

For the most updated list of papers, please see my [Google Scholar profile](https://scholar.google.com/citations?user=MRLe02cAAAAJ&hl=en).


**In-Context Learning with Transformers: Softmax Attention Adapts to Function Lipschitzness**  
*LC\*, Advait Parulekar\*, Aryan Mokhtari, Sujay Sanghavi, Sanjay Shakkottai*  
\* co-first authors  
NeurIPS 2024, Spotlight
[\[PDF\]](https://arxiv.org/pdf/2402.11639)

**Provable Multi-Task Representation Learning by Two-Layer ReLU Neural Networks**  
*LC, Hamed Hassani, Mahdi Soltanolkotabi, Aryan Mokhtari, Sanjay Shakkottai*  
ICML 2024 **Oral Presentation**
[\[PDF\]](https://arxiv.org/pdf/2307.06887.pdf)

**Profit: Benchmarking Personalization and Robustness Trade-off in Federated Prompt Tuning**  
*LC, Shanshan Wu, Sewoong Oh, Khe Chai Sim*  
Workshop on Federated Learning in the Age of Foundation Models in Conjunction with NeurIPS 2023  **Best Paper**  
[\[PDF\]](https://arxiv.org/pdf/2310.04627.pdf) 

**InfoNCE Provably Learns Cluster-Preserving Representations**  
*Advait Parulekar, LC, Karthikeyan Shanmugam, Aryan Mokhtari, Sanjay Shakkottai*  
COLT 2023  
[\[PDF\]](https://arxiv.org/pdf/2302.07920.pdf)

**FedAvg with Fine-Tuning: Local Updates Lead to Representation Learning**  
*LC, Hamed Hassani, Aryan Mokhtari, Sanjay Shakkottai*  
NeurIPS 2022     
[\[PDF\]](https://arxiv.org/pdf/2205.13692.pdf)

**PerFedSI: A Framework for Personalized Federated Learning with Side Information**  
*LC, Enmao Diao, Tanya Roosta, Jie Ding, Tao Zhang*  
Workshop on Federated Learning: Recent Advances and New Challenges in Conjunction with NeurIPS 2022  
[\[PDF\]](https://openreview.net/pdf?id=HRZjvFkX-faD) 

**MAML and ANIL Provably Learn Representations**  
*LC, Aryan Mokhtari, Sewoong Oh, Sanjay Shakkottai*  
ICML 2022     
[\[PDF\]](https://arxiv.org/pdf/2202.03483.pdf)

**How does the Task Landscape Affect MAML Performance?**  
*LC, Aryan Mokhtari, Sanjay Shakkottai*  
CoLLAs 2022 *Oral Presentation*   
[\[PDF\]](https://arxiv.org/pdf/2010.14672.pdf)

**Exploiting Shared Representations for Personalized Federated
Learning**  
*LC, Hamed Hassani, Aryan Mokhtari, Sanjay Shakkottai*  
ICML 2021    
[\[PDF\]](https://arxiv.org/pdf/2102.07078.pdf) [\[Code\]](https://github.com/lgcollins/FedRep)

**Task-Robust Model-Agnostic Meta-Learning**  
*LC, Aryan Mokhtari, Sanjay Shakkottai*  
NeurIPS 2020    
[\[PDF\]](https://arxiv.org/abs/2002.04766.pdf) [\[Code\]](https://github.com/lgcollins/tr-maml)
