---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

**About Me**

I am a fifth-year PhD student at the University of Texas at Austin co-advised by [Aryan Mokhtari](https://sites.utexas.edu/mokhtari/) and [Sanjay Shakkottai](https://sites.google.com/view/sanjay-shakkottai/home). Before UT, I completed my undergrad at Princeton University where I worked with [Yuxin Chen](https://yuxinchen2020.github.io/). I am broadly interested in many areas within AI/ML -- my research has explored topics including federated learning, meta-learning, multi-task learning, contrastive learning, and most recently, in-context learning and parameter-efficient fine-tuning of large language models (LLMs).

**I am graduating in May 2024 and am currently on the industry job market.**

My email is liamc at utexas dot edu.

<!---
My CV can be found [here](https://liamc2196.github.io/files/Liamc_CV_nov22.pdf) (updated 11/2022).
--->

## News

- **December 2023:** Our [paper](https://arxiv.org/pdf/2310.04627.pdf) was selected as a Best Paper at [FL@FM-NeurIPS’23](https://federated-learning.org/fl@fm-neurips-2023/)!

- **October 2023:** Our [paper](https://arxiv.org/pdf/2310.04627.pdf) on federated prompt tuning was selected for Oral Presentation at [FL@FM-NeurIPS’23](https://federated-learning.org/fl@fm-neurips-2023/).

- **Summer 2023:** I interned at Google Research, working with [Shanshan Wu](https://wushanshan.github.io/), [Sewoong Oh](https://homes.cs.washington.edu/~sewoong/), and [Khe Chai Sim](https://scholar.google.com/citations?user=jnU62sUAAAAJ&hl=en) on federated prompt tuning of large language models.

- **June 2023:** New [paper](https://arxiv.org/pdf/2307.06887.pdf) on multi-task learning with two-layer ReLU networks.

- **May 2023:** Our paper [InfoNCE Loss Provably Learns Cluster-Preserving Representations](https://arxiv.org/pdf/2302.07920.pdf) was accepted at COLT 2023.

- **October 2022:** I gave a [talk](https://sites.google.com/view/one-world-seminar-series-flow/archive/2022) on representation learning in federated learning at the Federated Learning One World (FLOW) Seminar.

- **Summer 2022:** I interned at Amazon Alexa under the supervision of [Jie Ding](https://jding.org/) and [Tanya Roosta](https://www.amazon.science/author/tanya-g-roosta). My project studied personalized federated learning with side information. Our [paper](https://openreview.net/forum?id=HRZjvFkX-faD) was accepted at [FL-NeurIPS'22](https://federated-learning.org/fl-neurips-2022/). 


## Papers

For the most updated list of papers, please see my [Google Scholar profile](https://scholar.google.com/citations?user=MRLe02cAAAAJ&hl=en).


<!---{% for post in site.publications reversed %}
          {% include archive-single.html %}
     {% endfor %}--->

**Profit: Benchmarking Personalization and Robustness Trade-off in Federated Prompt Tuning**  
*LC, Shanshan Wu, Sewoong Oh, Khe Chai Sim*  
FL@FM-NeurIPS'23  *Best Paper*
[\[PDF\]](https://arxiv.org/pdf/2310.04627.pdf)

**Provable Multi-Task Representation Learning by Two-Layer ReLU Neural Networks**  
*LC, Hamed Hassani, Mahdi Soltanolkotabi, Aryan Mokhtari, Sanjay Shakkottai*  
arxiv preprint  
[\[PDF\]](https://arxiv.org/pdf/2307.06887.pdf)
     
**InfoNCE Provably Learns Cluster-Preserving Representations**  
*Advait Parulekar, LC, Karthikeyan Shanmugam, Aryan Mokhtari, Sanjay Shakkottai*  
COLT 2023  
[\[PDF\]](https://arxiv.org/pdf/2302.07920.pdf)

**FedAvg with Fine-Tuning: Local Updates Lead to Representation Learning**  
*LC, Hamed Hassani, Aryan Mokhtari, Sanjay Shakkottai*  
NeurIPS 2022     
[\[PDF\]](https://arxiv.org/pdf/2205.13692.pdf)

**MAML and ANIL Provably Learn Representations**  
*LC, Aryan Mokhtari, Sewoong Oh, Sanjay Shakkottai*  
ICML 2022     
[\[PDF\]](https://arxiv.org/pdf/2202.03483.pdf)

**How does the Task Landscape Affect MAML Performance?**  
*LC, Aryan Mokhtari, Sanjay Shakkottai*  
CoLLAs 2022 *Oral Presentation*   
[\[PDF\]](https://arxiv.org/pdf/2010.14672.pdf)

**Exploiting Shared Representations for Personalized Federated
Learning**  
*LC, Hamed Hassani, Aryan Mokhtari, Sanjay Shakkottai*  
ICML 2021    
[\[PDF\]](https://arxiv.org/pdf/2102.07078.pdf) [\[Code\]](https://github.com/lgcollins/FedRep)

**Task-Robust Model-Agnostic Meta-Learning**  
*LC, Aryan Mokhtari, Sanjay Shakkottai*  
NeurIPS 2020    
[\[PDF\]](https://arxiv.org/abs/2002.04766.pdf) [\[Code\]](https://github.com/lgcollins/tr-maml)
